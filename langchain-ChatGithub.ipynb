{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMMRqkLK3bTOuxTH7Ck34Ew"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Using langchain and huggingface hub to develop a chatbot with a given Github repo"
   ],
   "metadata": {
    "id": "tnIs8X2fHmzM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LOzeeQzEVwW"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain deeplake huggingface_hub tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install sentence_transformers"
   ],
   "metadata": {
    "id": "7OVcuDreFc5L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# getting huggingface hub api token from https://huggingface.co/settings/token\n",
    "# getting active loop token from https://app.activeloop.ai/account\n",
    "# active loop provide deep lake service as an online database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your huggingface hub api token'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'your active loop token'"
   ],
   "metadata": {
    "id": "tnM4AhQ5EeqY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686901220377,
     "user_tz": -600,
     "elapsed": 321,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# the embeddings are used to generate vectors and store them in the database\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "id": "O46YnFMYFSUv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686901282632,
     "user_tz": -600,
     "elapsed": 501,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# introduce the target repo (as an example)\n",
    "!git clone https://github.com/chroma-core/chroma.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lhh6rujzFZxd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686900295073,
     "user_tz": -600,
     "elapsed": 6528,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "69e2b828-9acd-4f1b-9f23-4c1ff2ec7fbf"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'chroma'...\n",
      "remote: Enumerating objects: 14581, done.\u001B[K\n",
      "remote: Counting objects: 100% (3378/3378), done.\u001B[K\n",
      "remote: Compressing objects: 100% (932/932), done.\u001B[K\n",
      "remote: Total 14581 (delta 2617), reused 2864 (delta 2378), pack-reused 11203\u001B[K\n",
      "Receiving objects: 100% (14581/14581), 172.26 MiB | 38.38 MiB/s, done.\n",
      "Resolving deltas: 100% (9464/9464), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# index the data -> vectorize the data\n",
    "# Chroma - the open-source embedding database.\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "root_dir = './chroma'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir): # Load files under the git directory by iterating through them.\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath,file),encoding='utf-8') # textloader: load text from file\n",
    "            docs.extend(loader.load_and_split())  # get the text array from the file\n",
    "        except Exception as e:\n",
    "            pass\n",
    "print(len(docs))  # number of docs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4HVgTXNF6fv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686900896701,
     "user_tz": -600,
     "elapsed": 573,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "d924ef04-109f-4fa4-a746-76afccf8d342"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "517\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# split the docs array\n",
    "# split the docs array into chunks of 1000 characters with 0 overlap\n",
    "# the text splitter is used to split the text into chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(len(texts))  # the chunks of docs\n",
    "# the text loader will warn us if there are some files that are larger than our limit of 1000."
   ],
   "metadata": {
    "id": "ZynhOoVFGsmz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# generate a database under deep lake, and store the data in it, public=True means the dataset is public.\n",
    "username = 'nalanwutuo'\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/chroma\",embedding_function=hf_embeddings,public=True)\n",
    "db.add_documents(texts)"
   ],
   "metadata": {
    "id": "O632jlC2Hi9l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get data from the database\n",
    "db = DeepLake(dataset_path=\"hub://nalanwutuo/chroma\",read_only=True,embedding_function=hf_embeddings)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxlGfyoZJOK1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686901773955,
     "user_tz": -600,
     "elapsed": 4091,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "9e119b71-d162-4262-9706-f1023ed5ece4"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/nalanwutuo/chroma\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\\"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hub://nalanwutuo/chroma loaded successfully.\n",
      "\n",
      "Deep Lake Dataset in hub://nalanwutuo/chroma already exists, loading from the storage\n",
      "Dataset(path='hub://nalanwutuo/chroma', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (1386, 384)  float32   None   \n",
      "    ids      text     (1386, 1)     str     None   \n",
      " metadata    json     (1386, 1)     str     None   \n",
      "   text      text     (1386, 1)     str     None   \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r \r\r \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# set the parameters for the database query\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'  # similarity metric\n",
    "retriever.search_kwargs['fetch_k'] = 100  # fetch_k represents the number of documents to be fetched from the database for each query\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10  # k represents the number of documents to be returned for each query"
   ],
   "metadata": {
    "id": "VVtVRyaRLkQS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686901988627,
     "user_tz": -600,
     "elapsed": 3,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model setting, here we use flan-t5-large with huggingface hub api\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# initialize\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id = 'google/flan-t5-large',\n",
    "    model_kwargs = {\"temperature\":1e-10,\"max_length\":3000}\n",
    ")\n",
    "qa = ConversationalRetrievalChain.from_llm(flan_t5,retriever=retriever)"
   ],
   "metadata": {
    "id": "utGttkV8MZgW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686902280791,
     "user_tz": -600,
     "elapsed": 2,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"What does Chroma do?\",\n",
    "    \"How to use Chroma\"\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"Question:\\n {question} \\n\")\n",
    "    print(f\"Answer:\\n {result['answer']} \\n\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZm0NhQgNgyD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686902433621,
     "user_tz": -600,
     "elapsed": 63547,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "0fb70a13-6d5c-4809-b5d7-e3fa32bf0782"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question:\n",
      " What does Chroma do? \n",
      "\n",
      "Answer:\n",
      " embedding database \n",
      "\n",
      "\n",
      "Question:\n",
      " How to use Chroma \n",
      "\n",
      "Answer:\n",
      " To connect to your server and perform operations using the client only library, you can do the following: # requirements # - docker # - pip # get the code git clone https://oauth2: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@github.com/chroma-core/chroma.git #checkout the right branch cd chroma # run docker cd chroma-server docker-compose up -d --build # install chroma-client cd ../chroma-client pip3 install --upgrade pip # you have to do this or it will use UNKNOWN as the package name pip install \n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def ask(question, chat_history):\n",
    "    response = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    print(f\"Question:\\n {question}\\n\")\n",
    "    print(f\"Answer:\\n {response['answer']}\\n\")"
   ],
   "metadata": {
    "id": "hlwBnfQRN2vm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686902734450,
     "user_tz": -600,
     "elapsed": 512,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ask(\"What's the main programming language used in Chroma?\",chat_history)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiGGNVQOPPmk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686902777969,
     "user_tz": -600,
     "elapsed": 12922,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "d32c1f7f-ba7f-4993-81d8-6a659ee7a80c"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question:\n",
      " What's the main programming language used in Chroma?\n",
      "\n",
      "Answer:\n",
      " python\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ask(\"Summarize the storage part of Chroma\",chat_history)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEOruUmlPXFA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686902911369,
     "user_tz": -600,
     "elapsed": 87837,
     "user": {
      "displayName": "Andy Guo",
      "userId": "03938731780172859167"
     }
    },
    "outputId": "c61ec32e-5640-4c61-a717-7284da7409c0"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question:\n",
      " Summarize the storage part of Chroma\n",
      "\n",
      "Answer:\n",
      " disks> backups> type>local/type> path>/etc/clickhouse-server//path> /backups> /disks> /storage_configuration> backups> allowed_disk>backups/allowed_disk> allowed_path>/etc/clickhouse-server//allowed_path> /backups> /clickhouse> EOF\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qlA3N6uRPlWO"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
